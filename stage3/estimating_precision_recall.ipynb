{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep 1: steps to install the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py_entitymatching\n",
    "!pip install scipy\n",
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_entitymatching as em\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NEED MODIFICATION: Modify this cell to point to the file location]\n",
    "# Prep 2: enter the file location on your harddisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_a = 'TableA'\n",
    "table_b = 'TableB'\n",
    "candidate_set = 'CandidateSet'\n",
    "prediction_set = 'PredictionSet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep 3: reading the files into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "dfa = em.read_csv_metadata(table_a, key='_id')\n",
    "dfa['Name'] = dfa['Name'].str.lower()\n",
    "\n",
    "dfb = em.read_csv_metadata(table_b, key='_id')\n",
    "dfb['Name'] = dfb['Name'].str.lower()\n",
    "\n",
    "dfc = em.read_csv_metadata(candidate_set, key='_id',\n",
    "                           fk_ltable='A_id', fk_rtable='B_id',\n",
    "                           ltable=dfa, rtable=dfb)\n",
    "dfp = em.read_csv_metadata(prediction_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_f = em.get_features_for_blocking(dfa, dfb, validate_inferred_attr_types=False)\n",
    "block_f = block_f.iloc[[4],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/x-python"
   },
   "source": [
    "# Code for reducing density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_rule_0'"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb = em.RuleBasedBlocker()\n",
    "rb.add_rule(['Name_Name_jac_qgm_3_qgm_3(ltuple, rtuple) < 0.5'], block_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2 = rb.block_candset(dfc)\n",
    "len(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2.to_csv('C3.csv', index=False)\n",
    "C3 = 'C3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# C2.drop(columns=['_id'], inplace=True)\n",
    "densityFlag = False\n",
    "i = 0\n",
    "C = dfa\n",
    "L = []\n",
    "while densityFlag == False:\n",
    "    B = em.sample_table(C2, 50)\n",
    "    B.to_csv('B.csv', index=False)\n",
    "    densityFlag=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module: debug_blocker\n",
    "# Description: debug the blocking rule using the below script to ensure you are not dropping true matches\n",
    "# Note: You need to run Prep 1 and 2 in order to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input format:\n",
    "# Format of table_a:\n",
    "# _id, attribute1, attribute2, ....., attributen\n",
    "\n",
    "# Format of table_b:\n",
    "# _id, attribute1, attribute2, ....., attributen\n",
    "\n",
    "# Format of candidate_set\n",
    "# A_id,B_id\n",
    "# where A_id is _id from table_a and B_id is the _id column value from table_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "\n",
    "def run_debug_blocker(table_a, table_b, table_a_key, table_b_key, candidate_set):\n",
    "    dfl = em.read_csv_metadata(table_a, key=table_a_key)\n",
    "    dfr = em.read_csv_metadata(table_b, key=table_b_key)\n",
    "\n",
    "    # reading the candidate set and adding key\n",
    "    dfcand = pd.read_csv(candidate_set)\n",
    "    dfcand.drop_duplicates(inplace=True)\n",
    "    dfcand.to_csv('cand_set_with_index.csv', index_label='id')\n",
    "\n",
    "    dfcset = em.read_csv_metadata('cand_set_with_index.csv', key='id', ltable=dfl, \n",
    "                                  rtable=dfr, fk_ltable='A_id', fk_rtable='B_id')\n",
    "\n",
    "    # running debug blocker to identify the records in A x B \\ C\n",
    "    debug_file = em.debug_blocker(dfcset, dfl, dfr, output_size=200)\n",
    "    \n",
    "    return debug_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "debug_file = run_debug_blocker(table_a, table_b, '_id', '_id', candidate_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module: estimate_precision_recall\n",
    "# Description: the below code helps you get an estimation of P/R on the candidate set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from numpy import sqrt\n",
    "\n",
    "delta = .05\n",
    "Z = norm.ppf(1 - (delta / 2))\n",
    "\n",
    "def estimate_PR(labeled_pairs, reduced_cands, predicted_matches):\n",
    "    '''\n",
    "    labeled_pairs - a pandas dataframe with schema id1,id2,label\n",
    "                    Note label needs to be Boolean\n",
    "\n",
    "    reduced_cands - a pandas dataframe with schema id1,id2\n",
    "    predicted_matches - a pandas dataframe with schema id1,id2\n",
    "    \n",
    "    return:\n",
    "        ( (recall lower bound, recall upper bound), (precision lower bound, precision upper bound) )\n",
    "    '''\n",
    "\n",
    "    labeled_pairs.drop_duplicates(inplace=True)\n",
    "    labeled_pairs.columns = ['id1', 'id2', 'label']\n",
    "    reduced_cands.columns = ['id1', 'id2']\n",
    "    reduced_cand_set = set(zip(reduced_cands.id1, reduced_cands.id2))\n",
    "    predicted_matches = set(zip(predicted_matches.id1, predicted_matches.id2))\n",
    "    \n",
    "    # estimate the recall\n",
    "    # number of positives in the labeled sample\n",
    "    actual_pos = float(labeled_pairs.label.sum())\n",
    "    # the maximum number of postives in the candidate set\n",
    "    max_actual_pos = float(actual_pos + len(reduced_cand_set) - len(labeled_pairs))\n",
    "    \n",
    "    # true positives in the labeled sample\n",
    "    true_pos = float(labeled_pairs.apply(lambda x : (x['id1'], x['id2']) in predicted_matches and x['label'], axis=1).sum())\n",
    "    #estimated recall\n",
    "    recall = float(true_pos / actual_pos)\n",
    "\n",
    "    recall_error = Z * sqrt( ((recall * (1 - recall)) / (actual_pos)) * ((max_actual_pos - actual_pos) / (max_actual_pos - 1)) )\n",
    "\n",
    "\n",
    "    # estimate Precision\n",
    "    labeled_set  = set(zip(labeled_pairs.id1, labeled_pairs.id2))\n",
    "    predicted_pos = float(len(labeled_set & predicted_matches))\n",
    "    \n",
    "    predicted_pos_in_reduced_cand_set = float(len(reduced_cand_set & predicted_matches))\n",
    "    \n",
    "    alpha =  predicted_pos_in_reduced_cand_set / len(predicted_matches)\n",
    "    precision = alpha * (true_pos / predicted_pos)\n",
    "    \n",
    "    precision_error = alpha * Z * sqrt( ((precision * (1 - precision)) / predicted_pos) * (float((len(predicted_matches) - predicted_pos)) / (len(predicted_matches)  - 1)) )\n",
    "\n",
    "    return ((recall - recall_error, recall + recall_error),\n",
    "            (precision - precision_error, precision + precision_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NEED MODIFICATION: Modify this cell to point to the file location]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.983121531366644, 0.9932796485743588), (0.9828007156935782, 0.9925687424345006))\n"
     ]
    }
   ],
   "source": [
    "# read the labeled pairs file, i.e. the file with the labels\n",
    "red_cand_set = 'reduced_cands.csv'\n",
    "# dfc = em.read_csv_metadata(candidate_set)\n",
    "dfc = em.read_csv_metadata(red_cand_set,\n",
    "                           fk_ltable='id1', fk_rtable='id2',\n",
    "                           ltable=dfa, rtable=dfb)\n",
    "\n",
    "\n",
    "labeled_pairs = em.read_csv_metadata('labeled_pairs.csv')\n",
    "print(estimate_PR(labeled_pairs, dfc, dfp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatively if you run into issues running this script on your laptop you can use\n",
    "# https://colab.research.google.com/notebooks/welcome.ipynb"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
